#! /usr/bin/env python
# -*- coding: utf-8 -*-

"""
Image deformation using moving least squares.

    * Affine deformation
    * Similarity deformation
    * Rigid deformation

For more details please refer to the Chinese documentation: 
    
    ./doc/Image Deformation.pdf

or the original paper: 
    
    Image deformation using moving least squares
    Schaefer, Mcphail, Warren. 

Note:
    In the original paper, the author missed the weight w_j in formular (5).
    In addition, all the formulars in section 2.1 miss the w_j. 
    And I have corrected this point in my documentation.

@author: Jian-Wei ZHANG
@email: zjw.cs@zju.edu.cn
@date: 2017/8/8
@update: 2020/9/25: No need for so-called inverse transformation. Just transform target pixels to the corresponding source pixels.
@update: 2021/7/14: Optimize memory usage. Now a 2000x2000 image with 64 control points spend about 4.2GB memory. (20GB in the previous version)
@update: 2021/12/24: Fix a bug of nan values in `mls_rigid_deformation()`. (see issue #13)
"""

import numpy as np
import torch
import torch.nn.functional as F

np.seterr(divide="ignore", invalid="ignore")


def mls_similarity_deformation(vy, vx, p, q, alpha=1.0, eps=1e-8):
    """Similarity deformation

    Parameters
    ----------
    vx, vy: ndarray
        coordinate grid, generated by np.meshgrid(gridX, gridY)
    p: ndarray
        an array with size [n, 2], original control points, in (y, x) formats
    q: ndarray
        an array with size [n, 2], final control points, in (y, x) formats
    alpha: float
        parameter used by weights
    eps: float
        epsilon

    Return
    ------
        A deformed image.
    """

    q = q.type(torch.int16).contiguous()
    p = p.type(torch.int16).contiguous()

    # Exchange p and q and hence we transform destination pixels to the corresponding source pixels.
    p, q = q, p

    grow = vx.shape[0]  # grid rows
    gcol = vx.shape[1]  # grid cols
    ctrls = p.shape[0]  # control points

    # Compute
    reshaped_p = torch.tensor(p.reshape(ctrls, 2, 1, 1)).cuda()  # [ctrls, 2, 1, 1]
    reshaped_v = torch.tensor(
        np.vstack((vx.reshape(1, grow, gcol), vy.reshape(1, grow, gcol)))
    ).cuda()  # [2, grow, gcol]

    w = (
        1.0 / (torch.sum((reshaped_p - reshaped_v).type(torch.float32) ** 2, axis=1) + eps) ** alpha
    )  # [ctrls, grow, gcol]
    w /= torch.sum(w, axis=0, keepdims=True)  # [ctrls, grow, gcol]

    # 矩阵式
    pstar1 = w * reshaped_p[:, 0, :, :]
    pstar2 = w * reshaped_p[:, 1, :, :]
    pstar = torch.stack([pstar1.sum(0), pstar2.sum(0)], dim=0)

    phat = reshaped_p - pstar  # [ctrls, 2, grow, gcol]
    reshaped_phat = phat.reshape(ctrls, 1, 2, grow, gcol)  # [ctrls, 1, 2, grow, gcol]
    reshaped_w = w.reshape(ctrls, 1, 1, grow, gcol)  # [ctrls, 1, 1, grow, gcol]

    # 矩阵式
    phat_2 = (phat**2).sum(1)
    mu = (w * phat_2).sum(0)

    reshaped_mu = mu.reshape(1, grow, gcol)  # [1, grow, gcol]

    vpstar = reshaped_v - pstar  # [2, grow, gcol]
    reshaped_vpstar = vpstar.reshape(2, 1, grow, gcol)  # [2, 1, grow, gcol]
    neg_vpstar_verti = vpstar[[1, 0], ...]  # [2, grow, gcol]
    neg_vpstar_verti[1, ...] = -neg_vpstar_verti[1, ...]
    reshaped_neg_vpstar_verti = neg_vpstar_verti.reshape(2, 1, grow, gcol)  # [2, 1, grow, gcol]
    mul_right = torch.cat([reshaped_vpstar, reshaped_neg_vpstar_verti], dim=1)  # [2, 2, grow, gcol]

    # Calculate q
    reshaped_q = torch.tensor(q.reshape((ctrls, 2, 1, 1))).cuda()  # [ctrls, 2, 1, 1]
    qstar = torch.zeros((2, grow, gcol), dtype=torch.float32).cuda()
    for i in range(ctrls):
        qstar += w[i] * reshaped_q[i]  # [2, grow, gcol]

    # Get final image transfomer -- 3-D array
    temp = torch.zeros((grow, gcol, 2), dtype=torch.float32).cuda()

    # 矩阵式
    sid = 0
    eid = 8 if ctrls > 8 else ctrls
    hold_qstar = qstar.clone()
    qstar = qstar.repeat(eid - sid, 1, 1, 1)
    mul_right = mul_right.repeat(eid - sid, 1, 1, 1, 1)
    while eid <= ctrls:
        phat_ten = phat[sid:eid].cuda()
        neg_phat_verti = phat_ten[:, [1, 0]]  # [ctrls,2, grow, gcol]
        neg_phat_verti[:, 1] = -neg_phat_verti[:, 1]
        reshaped_neg_phat_verti = neg_phat_verti.reshape(eid - sid, 1, 2, grow, gcol)  # [ctrls,1, 2, grow, gcol]
        mul_left = torch.cat((reshaped_phat[sid:eid], reshaped_neg_phat_verti), axis=1)  # [ctrls,2, 2, grow, gcol]
        A = torch.matmul(
            (reshaped_w[sid:eid] * mul_left).permute(0, 3, 4, 1, 2),
            mul_right[0 : eid - sid].permute(0, 3, 4, 1, 2),
        )  # [ctrls,grow, gcol, 2, 2]
        qhat = reshaped_q[sid:eid] - qstar[0 : eid - sid]  # [ctrls,2, grow, gcol]
        reshaped_qhat = qhat.reshape(eid - sid, 1, 2, grow, gcol).permute(0, 3, 4, 1, 2)  # [ctrls,grow, gcol, 1, 2]
        # Get final image transfomer -- 3-D array
        temp += torch.matmul(reshaped_qhat, A).sum(0).reshape(grow, gcol, 2)  # [grow, gcol, 2]
        sid = eid
        if eid == ctrls:
            break
        eid = eid + 8 if ctrls > 8 + eid else ctrls

    transformers = temp.permute(2, 0, 1) / reshaped_mu + hold_qstar  # [2, grow, gcol]

    # Removed the points outside the border
    transformers[transformers < 0] = 0
    transformers[0][transformers[0] > grow - 1] = 0
    transformers[1][transformers[1] > gcol - 1] = 0

    return transformers


# p控制点变化前坐标，q控制点变化后坐标
# mode：AFFINE仿射变换，SIMILARITY相似变换（仅包含平移、旋转和一致的放缩），RIGID刚性变换（仅包含平移和旋转）
# image原图片，vy和vx是meshgrid()生成的原图像像素坐标矩阵
def mls_deformation(p, q, vy, vx):
    p = torch.tensor(np.array(p)).reshape(-1, 2).cuda()
    p[:, [0, 1]] = p[:, [1, 0]]
    q = torch.tensor(np.array(q)).cuda()
    q[:, [0, 1]] = q[:, [1, 0]]
    transform = mls_similarity_deformation(vy, vx, p, q, alpha=1)

    return transform
